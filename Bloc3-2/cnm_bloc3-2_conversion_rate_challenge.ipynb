{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloc 3 - Analyse prédictive de données structurées par l'intelligence artificielle - Conversion rate challenge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "www.datascienceweekly.org is a free newsletter helping people keeping up with the latest developments in data science. It is curated by independent data scientists and anyone can simply register on the website with an email address to receive weekly news.\n",
    "\n",
    "### Problematic\n",
    "\n",
    "wwww.datascienceweekly.org aims at improving the newsletter's conversion rate.\n",
    "\n",
    "The creators of the newsletter would like to better understand the behavior of the users visiting their website. \n",
    "\n",
    "### Scope\n",
    "\n",
    "To improve the number of subscribers to their newsletter, the creators of the newsletter designed a competion to get the best predictive model for conversion (using f1-score for rankings). They open-sourced a dataset containing information about the traffic on their website.\n",
    "\n",
    "Competitors were asked to develop the best model for prediction and to analyse the parameters of their model to identify features that would explain the users behavior in respect to conversion.\n",
    "\n",
    "### Aim and objectives\n",
    "\n",
    "Overall aim: Build a machine learning model able to estimate the conversion rate and identify levers of action.\n",
    "\n",
    "Objectives:\n",
    "- 1 - Make an EDA and preprocess data for machine learning.\n",
    "- 2 - Train a logistic regression model (baseline) and assess its performance.\n",
    "- 3 - Improve the model's f1-score\n",
    "- 4 - Make predictions on the unlabelled test set\n",
    "- 5 - Make recommendations to improve the conversion rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Methods\n",
    "\n",
    "### 1 - Library import\n",
    "\n",
    "### 2 - File reading and basic exploration\n",
    "\n",
    "The dataset was composed of records for 284580 visitors of the website. It contained 6 features: the country of the readers, their age, whether they are a new user or not, the traffic source, the total number of pages visited, and finally the target variable, conversion. \n",
    "\n",
    "The initial inspection of the dataset revealed that it did not contain any missing value, and that the age variable did contain outliers.\n",
    "\n",
    "### 3 - Exploratory data analysis\n",
    "\n",
    "As the target variable was categorical, probability distributions for each variable and for subscribers and non-subscribers were plotted.\n",
    "\n",
    "### 4 - Baseline model (linear regression) with one feature\n",
    "\n",
    "From the EDA, \"total_pages_visited\" was identified as the most useful feature. A baseline model was trained by using at first only this feature using a simple (univariate) logistic regression. Its performances were assessed.\n",
    "\n",
    "### 5 - Optimized model\n",
    "\n",
    "Firstly, rows containing outliers in the age variable (age above mean + 3 * std) were droped (1017 rows). Then, a step of feature engineering was included. Features were created from the age and the number of pages variables. Features were also created from combinations of two original features. The resulting dataset contained 283563 rows for 136 features.\n",
    "\n",
    "Secondly, data was processed for machine learning and feature selection was performed using a strategy of forward sequential selection (with optional exclusion of already selected features at each iteration).\n",
    "\n",
    "Finally, a logistic regression model was trained with the best combination of features, and its performances assesed after fine-tuning of its regularization strength.\n",
    "\n",
    "### 6 - Predictions on test data\n",
    "\n",
    "The entire train dataset and the unlabelled test dataset were preprocessed as done for the model optimization, the optimized model was then trained and predictions were made form the unlabelled test data.\n",
    "\n",
    "### 7 - Feature importance\n",
    "\n",
    "The feature importance of the optimized model was displayed from the model coefficients."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Conclusion\n",
    "\n",
    "The baseline model for the prediction of the conversion rate could be improved (with regards to the f1-score). A single feature was identified as the main factor influencing the subscription to the newsletter.\n",
    "\n",
    "The f1-scores obtained from the baseline model (univariate logistic regression) were 0.6938 for the train set and 0.7060 for the test set. After cleaning of the data, engineering of new features, feature selection and fine-tuning, theses scores could be increased to 0.7652 for the train set and 0.7712 for the test set (optimized model).\n",
    "\n",
    "Regarding feature importance, the most important feature by far for conversion rate prediction was the total number of pages visited by the readers. Readers that visit many pages are more likely to subscribe to the newsletter.\n",
    "\n",
    "To increase the number of subscriptions to their newsletter, the team of www.datascienceweekly.org could try to increase the traffic between the different pages of their website. For example, they could highlight the most read content on their home page and make content suggestions based on the pages that were already visited by the reader. Since the subscription form is already present on all the pages of their website, this strategy could encourage readers to subscribe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 - library import ### ----\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from feature_engine.selection import DropDuplicateFeatures\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 2 - File reading and basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - import dataset ### ----\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv(\"cnm_bloc3-2_data_train.csv\")\n",
    "data_test = pd.read_csv(\"cnm_bloc3-2_data_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get basic stats ### ----\n",
    "\n",
    "# print shape of data\n",
    "print(\"Number of rows: {}\".format(data.shape[0]))\n",
    "print(\"Number of columns: {}\".format(data.shape[1]))\n",
    "print()\n",
    "\n",
    "# display dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Dataset display: \")\n",
    "display(data.head())\n",
    "print()\n",
    "\n",
    "# display basic statistics\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = data.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get percentage of missing values ### ----\n",
    "\n",
    "# check wether some columns are full of NaNs\n",
    "column_nan_full = data.columns[data.isnull().all()]\n",
    "column_nb = len(column_nan_full)\n",
    "\n",
    "# get percentage of missing values in columns\n",
    "percent_nan_col = data.isnull().sum() / data.shape[0] * 100\n",
    "\n",
    "# check wether some rows are full of NaNs\n",
    "row_nan_count = pd.Series([data.loc[i,:].isnull().sum() for i in range(0, data.shape[0])])\n",
    "row_nan_full = row_nan_count.index[row_nan_count == data.shape[1]]\n",
    "row_nb = len(row_nan_full)\n",
    "\n",
    "# print report\n",
    "print(\"COLUMNS\")\n",
    "print(\"{} columns out of {} are fully filled with missing values\".format(column_nb,data.shape[1]))\n",
    "print(\"Percentage of missing values per column:\\n{}\".format(percent_nan_col))\n",
    "print()\n",
    "print(\"ROWS\")\n",
    "print(\"{} rows out of {} are fully filled with missing values\".format(row_nb,data.shape[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 3 - Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - exploratory data analysis - sample dataset ### ----\n",
    "\n",
    "# the dataset is quite big: create a sample of the dataset before making any visualizations\n",
    "data_sample = data.sample(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - exploratory data analysis - plot univariate analysis ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig1 = make_subplots(\n",
    "    rows = 2,\n",
    "    cols = 3,\n",
    "    subplot_titles = (\n",
    "        \"A. Age\",\n",
    "        \"B. Total pages visited\",\n",
    "        \"\",\n",
    "        \"D. Country\",\n",
    "        \"E. New user\",\n",
    "        \"F. Source\"),\n",
    "    column_widths = [0.25, 0.25, 0.25],\n",
    "    horizontal_spacing = 0.15)\n",
    "\n",
    "# plot distribution of age\n",
    "fig1.add_trace(go.Histogram(\n",
    "        name = \"No conversion\",\n",
    "        x = data_sample.loc[data[\"converted\"] == 0,\"age\"],\n",
    "        histnorm = 'probability',\n",
    "        marker_color = px.colors.qualitative.Vivid[0],\n",
    "        showlegend = True),\n",
    "        row = 1, col = 1)\n",
    "fig1.add_trace(go.Histogram(\n",
    "        name = \"Conversion\",\n",
    "        x = data_sample.loc[data[\"converted\"] == 1,\"age\"],\n",
    "        histnorm = 'probability',\n",
    "        marker_color = px.colors.qualitative.Vivid[1],\n",
    "        showlegend = True),\n",
    "        row = 1, col = 1)\n",
    "\n",
    "# plot distribution of total pages visited\n",
    "fig1.add_trace(go.Histogram(\n",
    "        name = \"No conversion\",\n",
    "        x = data_sample.loc[data[\"converted\"] == 0,\"total_pages_visited\"],\n",
    "        histnorm = 'probability',\n",
    "        marker_color = px.colors.qualitative.Vivid[0],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 2)\n",
    "fig1.add_trace(go.Histogram(\n",
    "        name = \"Conversion\",\n",
    "        x = data_sample.loc[data[\"converted\"] == 1,\"total_pages_visited\"],\n",
    "        histnorm = 'probability',\n",
    "        marker_color = px.colors.qualitative.Vivid[1],\n",
    "        showlegend = False),\n",
    "        row = 1, col = 2)\n",
    "\n",
    "# plot categorical variables\n",
    "features_cat = [\"country\", \"new_user\", \"source\"]\n",
    "[fig1.add_trace(go.Histogram(\n",
    "        name = \"No conversion\",\n",
    "        x = data_sample.loc[data[\"converted\"] == 0,features_cat[i]],\n",
    "        histnorm = 'probability',\n",
    "        marker_color = px.colors.qualitative.Vivid[0],\n",
    "        showlegend = False),\n",
    "        row = 2, col = i+1) for i in [0, 1, 2]]\n",
    "[fig1.add_trace(go.Histogram(\n",
    "        name = \"Conversion\",\n",
    "        x = data_sample.loc[data[\"converted\"] == 1,features_cat[i]],\n",
    "        histnorm = 'probability',\n",
    "        marker_color = px.colors.qualitative.Vivid[1],\n",
    "        showlegend = False),\n",
    "        row = 2, col = i+1) for i in [0, 1, 2]]\n",
    "\n",
    "\n",
    "# update layout\n",
    "fig1.update_annotations(font_size = 15)\n",
    "fig1.update_xaxes(tickfont = dict(size = 10))\n",
    "fig1.update_yaxes(title = \"Probability\", tickfont = dict(size = 10))\n",
    "fig1.update_layout(\n",
    "        margin = dict(l = 90, t= 120),\n",
    "        title_text = \"Figure 1. Univariate analysis\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        xaxis = dict(title = \"Age (Years)\"),\n",
    "        xaxis2 = dict(title = \"Total pages visited (Count)\"),\n",
    "        xaxis4 = dict(title = \"Country\"),\n",
    "        xaxis5 = dict(title = \"New user\"),\n",
    "        xaxis6 = dict(title = \"Source\"),\n",
    "        yaxis = dict(range = [0, 0.1], tickvals = [0, 0.02, 0.04, 0.06, 0.08]),\n",
    "        yaxis2 = dict(range = [0, 0.19], tickvals = [0, 0.05, 0.10, 0.15]),\n",
    "        yaxis4 = dict(range = [0, 1], tickvals = [0, 0.2, 0.4, 0.6, 0.8]),\n",
    "        yaxis5 = dict(range = [0, 1], tickvals = [0, 0.2, 0.4, 0.6, 0.8]),\n",
    "        yaxis6 = dict(range = [0, 0.75], tickvals = [0, 0.2, 0.4, 0.6]),\n",
    "        legend = dict(\n",
    "            yanchor = \"top\",\n",
    "            y = 0.83,\n",
    "            xanchor = \"left\",\n",
    "            x = 0.77,\n",
    "            font = dict(size = 11)),\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 600)\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 4 - Baseline model (logistic regression) with one feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - baseline model (logistic regression) with one feature - preprocess data for machine learning ### ----\n",
    "\n",
    "# separate target variable Y from features X\n",
    "X = data[\"total_pages_visited\"]\n",
    "Y = data[\"converted\"]\n",
    "\n",
    "# divide dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X.array.reshape(-1, 1), Y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# scale numeric feature\n",
    "featureencoder = StandardScaler()\n",
    "X_train = featureencoder.fit_transform(X_train)\n",
    "X_test = featureencoder.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - baseline model (logistic regression) with one feature - train model and asses performance ### ----\n",
    "\n",
    "# train model\n",
    "classifier1 = LogisticRegression()\n",
    "classifier1.fit(X_train, Y_train)\n",
    "\n",
    "# perform 5-fold cross-validation to evaluate the generalized f1-score \n",
    "scores1 = cross_val_score(classifier1, X_train, Y_train, scoring = \"f1\", cv = 5)\n",
    "print('Cross-validated f1-score: ', scores1.mean())\n",
    "print('Standard deviation: ', scores1.std())\n",
    "print()\n",
    "\n",
    "# make predictions on train and test sets\n",
    "Y_train_pred1 = classifier1.predict(X_train)\n",
    "Y_test_pred1 = classifier1.predict(X_test)\n",
    "\n",
    "# assess performance and print report\n",
    "f1_score_train1 = f1_score(Y_train, Y_train_pred1)\n",
    "f1_score_test1 = f1_score(Y_test, Y_test_pred1)\n",
    "print(\"f1-score on train set: \", f1_score_train1)\n",
    "print(\"f1-score on test set: \", f1_score_test1)\n",
    "print()\n",
    "print(\"Confusion matrix on train set:\")\n",
    "print(confusion_matrix(Y_train, Y_train_pred1))\n",
    "print()\n",
    "print(\"Confusion matrix on test set:\")\n",
    "print(confusion_matrix(Y_test, Y_test_pred1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 5 - Optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - optimized model - drop rows containing outliers ### ----\n",
    "\n",
    "# copy data for safety\n",
    "data1 = data.copy()\n",
    "\n",
    "# rename content of \"new_user\" (for automatic detection as categorical feature)\n",
    "data1[\"new_user\"] = data1[\"new_user\"].apply(lambda x: \"No\" if x == 0 else \"Yes\")\n",
    "\n",
    "# set bounds to identify outliers\n",
    "column_mean = data1[\"age\"].mean()\n",
    "column_std = data1[\"age\"].std()\n",
    "upper_bond = column_mean + 3 * column_std\n",
    "\n",
    "# get index of rows to drop\n",
    "mask_drop = data1[\"age\"] > upper_bond\n",
    "index_drop = data1.index[mask_drop]\n",
    "\n",
    "# drop rows\n",
    "data1 = data1.drop(index_drop, axis = 0)\n",
    "\n",
    "# count rows that were droped\n",
    "drop_nb = len(index_drop)\n",
    "\n",
    "# print report\n",
    "print(\"Number of rows with outliers that were dropped: {}\".format(drop_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - optimized model - engineer features ### ----\n",
    "\n",
    "# part 1 - create features from \"age\" and \"total_pages_visited\"\n",
    "\n",
    "data1[\"age_2\"] = data1[\"age\"]**2\n",
    "data1[\"age_3\"] = data1[\"age\"]**3\n",
    "data1[\"age_4\"] = data1[\"age\"]**4\n",
    "data1[\"age_5\"] = 1 / data1[\"age\"]\n",
    "data1[\"age_6\"] = 1 / data1[\"age\"]**2\n",
    "\n",
    "data1[\"page_2\"] = data1[\"total_pages_visited\"]**2\n",
    "data1[\"page_3\"] = data1[\"total_pages_visited\"]**3\n",
    "data1[\"page_4\"] = data1[\"total_pages_visited\"]**4\n",
    "data1[\"page_5\"] = 1 / data1[\"total_pages_visited\"]\n",
    "data1[\"page_6\"] = 1 / data1[\"total_pages_visited\"]**2\n",
    "\n",
    "# part 2 - create combinations of two features\n",
    "\n",
    "# create masks\n",
    "mask1 = data1[\"country\"] == \"US\"\n",
    "mask2 = data1[\"country\"] == \"UK\"\n",
    "mask3 = data1[\"country\"] == \"Germany\"\n",
    "mask4 = data1[\"country\"] == \"China\"\n",
    "\n",
    "mask5 = data1[\"age\"] < 26\n",
    "mask6 = (data1[\"age\"] >= 26) & (data[\"age\"] <= 46)\n",
    "mask7 = data1[\"age\"] > 46\n",
    "\n",
    "mask8 = data1[\"new_user\"] == \"No\"\n",
    "mask9 = data1[\"new_user\"] == \"Yes\"\n",
    "\n",
    "mask10 = data1[\"total_pages_visited\"] < 5\n",
    "mask11 = (data1[\"total_pages_visited\"] >= 5) & (data[\"total_pages_visited\"] <= 12)\n",
    "mask12 = data1[\"total_pages_visited\"] > 12\n",
    "\n",
    "mask13 = data1[\"source\"] == \"Ads\"\n",
    "mask14 = data1[\"source\"] == \"Seo\"\n",
    "mask15 = data1[\"source\"] == \"Direct\"\n",
    "\n",
    "# create dataframe containing mask names\n",
    "mask_ids = [\"mask1\", \"mask2\", \"mask3\", \"mask4\", \"mask5\", \"mask6\", \"mask7\", \"mask8\", \"mask9\", \"mask10\", \"mask11\",\n",
    "    \"mask12\", \"mask13\", \"mask14\", \"mask15\"]\n",
    "mask_names = [\"US\", \"UK\", \"Germany\", \"China\", \"young\", \"midage\", \"old\", \"old_user\", \"new_user\", \"few\", \"mid\", \n",
    "    \"many\", \"Ads\", \"Seo\", \"Direct\"]\n",
    "\n",
    "mask_ids = pd.DataFrame(mask_ids, columns = [\"mask\"])\n",
    "mask_names = pd.DataFrame(mask_names, columns = [\"name\"])\n",
    "masks = pd.concat([mask_ids, mask_names], axis = 1)\n",
    "\n",
    "# create combinations of features\n",
    "for i in range(0, masks.shape[0]):\n",
    "\n",
    "    mask_first = globals()[masks.loc[i,\"mask\"]]\n",
    "    \n",
    "    for j in range(i, masks.shape[0]):\n",
    "\n",
    "        mask_second = globals()[masks.loc[j,\"mask\"]]\n",
    "        \n",
    "        mask_current = mask_first | mask_second\n",
    "        feature_name = masks.loc[i,\"name\"] + \"_or_\" + masks.loc[j,\"name\"]\n",
    "\n",
    "        feature_current = pd.Series(\"no\", index = data1.index, name = feature_name)\n",
    "        feature_current[mask_current] = \"yes\"\n",
    "        data1 = pd.concat([data1, feature_current], axis = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - optimized model - preprocess data for machine learning ### ----\n",
    "\n",
    "# separate target variable Y from features X\n",
    "X2 = data1.drop([\"converted\"], axis = 1)\n",
    "Y2 = data1[\"converted\"]\n",
    "\n",
    "# drop duplicated features\n",
    "transformer2 = DropDuplicateFeatures()\n",
    "transformer2.fit(X2)\n",
    "X2 = transformer2.transform(X2)\n",
    "\n",
    "# get features by category\n",
    "features_all2 = X2.columns\n",
    "features_num2 = X2._get_numeric_data().columns\n",
    "features_cat2 = features_all2.drop(features_num2)\n",
    "\n",
    "# divide dataset into train and test sets\n",
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X2, Y2, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# create preprocessor object from pipelines for numeric and categorical features\n",
    "numeric_transformer2 = Pipeline(steps = [\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer2 = Pipeline(steps = [\n",
    "    (\"encoder\", OneHotEncoder(drop = \"first\", sparse_output = False))\n",
    "])\n",
    "preprocessor2 = ColumnTransformer(transformers = [\n",
    "    (\"num\", numeric_transformer2, features_num2),\n",
    "    (\"cat\", categorical_transformer2, features_cat2)\n",
    "])\n",
    "preprocessor2 = preprocessor2.set_output(transform = \"pandas\")\n",
    "\n",
    "# scale numeric features and encode categorical features\n",
    "X_train2 = preprocessor2.fit_transform(X_train2)\n",
    "X_test2 = preprocessor2.transform(X_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - optimized model - select features ### ----\n",
    "\n",
    "# select features sequentially (forward, floating variant)\n",
    "sffs = SFS(\n",
    "    LogisticRegression(max_iter = 2000), \n",
    "    k_features = 15,\n",
    "    forward = True,\n",
    "    floating = True, \n",
    "    scoring = 'f1',\n",
    "    cv = StratifiedKFold(5),\n",
    "    n_jobs = -2)\n",
    "sffs = sffs.fit(X_train2, Y_train2)\n",
    "\n",
    "# store results in a dataframe\n",
    "sffs_results = pd.DataFrame.from_dict(sffs.get_metric_dict()).T\n",
    "\n",
    "# plot results\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(go.Scatter(\n",
    "    x = sffs_results.index,\n",
    "    y = sffs_results[\"avg_score\"],\n",
    "    mode = \"lines+markers\",\n",
    "    marker_color = px.colors.qualitative.Vivid[1],\n",
    "    error_y = dict(array = sffs_results[\"std_dev\"], visible = True)\n",
    "))\n",
    "\n",
    "# update layout\n",
    "fig2.update_xaxes(tickfont = dict(size = 10))\n",
    "fig2.update_yaxes(tickfont = dict(size = 10))\n",
    "fig2.update_layout(\n",
    "        margin = dict(l = 120, t= 100),\n",
    "        title_text = \"Figure 2. Sequential Floating Forward Selection\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        xaxis = dict(title = \"Number of features\", range = [0, 16], tickvals = np.arange(16), showgrid = False,\n",
    "            zeroline = False),\n",
    "        yaxis = dict(title = \"Performance (f1-score)\", range = [0.699, 0.781], \n",
    "            tickvals = [0.70, 0.72, 0.74, 0.76, 0.78]),\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - optimized model - train model and asses performance ### ----\n",
    "\n",
    "# select features and update train and test sets\n",
    "features_selected = list(sffs_results.loc[7,\"feature_names\"])\n",
    "# use the following commented command if you do not want to run the previous cell (about 80 minutes to run)\n",
    "#features_selected = ['num__age_5','num__page_5','cat__US_or_young_yes','cat__China_or_China_yes',\n",
    "# 'cat__old_or_Direct_yes','cat__old_user_or_few_yes','cat__old_user_or_mid_yes']\n",
    "X_train2 = X_train2[features_selected]\n",
    "X_test2 = X_test2[features_selected]\n",
    "\n",
    "# tune c (inverse of regularization strength) with gridsearch\n",
    "params = {\n",
    "    'C': [0.1, 0.5, 1, 5, 10],\n",
    "}\n",
    "gridsearch = GridSearchCV(LogisticRegression(), param_grid = params, cv = 5, scoring = \"f1\", n_jobs = -2)\n",
    "gridsearch.fit(X_train2, Y_train2)\n",
    "print(\"Best hyperparameter: \", gridsearch.best_params_)\n",
    "print()\n",
    "\n",
    "# perform 5-fold cross-validation to evaluate the generalized f1-score \n",
    "scores2 = cross_val_score(LogisticRegression(C = gridsearch.best_params_[\"C\"]), X_train2, Y_train2, \n",
    "    scoring = \"f1\", cv = 5)\n",
    "print('Cross-validated f1-score: ', scores2.mean())\n",
    "print('Standard deviation: ', scores2.std())\n",
    "print()\n",
    "\n",
    "# train model\n",
    "classifier2 = LogisticRegression(C = gridsearch.best_params_[\"C\"])\n",
    "classifier2.fit(X_train2, Y_train2)\n",
    "\n",
    "# make predictions on train and test sets\n",
    "Y_train_pred2 = classifier2.predict(X_train2)\n",
    "Y_test_pred2 = classifier2.predict(X_test2)\n",
    "\n",
    "# assess performance and print report\n",
    "f1_score_train2 = f1_score(Y_train2, Y_train_pred2)\n",
    "f1_score_test2 = f1_score(Y_test2, Y_test_pred2)\n",
    "print(\"f1-score on train set: \", f1_score_train2)\n",
    "print(\"f1-score on test set: \", f1_score_test2)\n",
    "print()\n",
    "print(\"Confusion matrix on train set:\")\n",
    "print(confusion_matrix(Y_train2, Y_train_pred2))\n",
    "print()\n",
    "print(\"Confusion matrix on test set:\")\n",
    "print(confusion_matrix(Y_test2, Y_test_pred2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 6 - Predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6 - predictions on test data - print selected features ### ----\n",
    "\n",
    "print(\"Selected features:\\n{}\".format(sffs_results.loc[7,\"feature_names\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6 - predictions on test data - preprocess test data ### ----\n",
    "\n",
    "# part 1 - create features from \"age\" and \"total_pages_visited\"\n",
    "\n",
    "data_test[\"age_5\"] = 1 / data_test[\"age\"]\n",
    "data_test[\"page_5\"] = 1 / data_test[\"total_pages_visited\"]\n",
    "\n",
    "\n",
    "# part 2 - create combinations of two features\n",
    "\n",
    "# create masks\n",
    "mask1 = data_test[\"country\"] == \"US\"\n",
    "mask4 = data_test[\"country\"] == \"China\"\n",
    "\n",
    "mask5 = data_test[\"age\"] < 26\n",
    "mask7 = data_test[\"age\"] > 46\n",
    "\n",
    "mask8 = data_test[\"new_user\"] == \"No\"\n",
    "\n",
    "mask10 = data_test[\"total_pages_visited\"] < 5\n",
    "mask11 = (data_test[\"total_pages_visited\"] >= 5) & (data_test[\"total_pages_visited\"] <= 12)\n",
    "\n",
    "mask15 = data_test[\"source\"] == \"Direct\"\n",
    "\n",
    "# create dataframe containing mask names\n",
    "mask_ids = [\"mask1\", \"mask4\", \"mask5\", \"mask7\", \"mask8\", \"mask10\", \"mask11\", \"mask15\"]\n",
    "mask_names = [\"US\", \"China\", \"young\", \"old\", \"old_user\", \"few\", \"mid\", \"Direct\"]\n",
    "\n",
    "mask_ids = pd.DataFrame(mask_ids, columns = [\"mask\"])\n",
    "mask_names = pd.DataFrame(mask_names, columns = [\"name\"])\n",
    "masks = pd.concat([mask_ids, mask_names], axis = 1)\n",
    "\n",
    "# create combinations of features\n",
    "for i in range(0, masks.shape[0]):\n",
    "\n",
    "    mask_first = globals()[masks.loc[i,\"mask\"]]\n",
    "    \n",
    "    for j in range(i, masks.shape[0]):\n",
    "\n",
    "        mask_second = globals()[masks.loc[j,\"mask\"]]\n",
    "        \n",
    "        mask_current = mask_first | mask_second\n",
    "        feature_name = masks.loc[i,\"name\"] + \"_or_\" + masks.loc[j,\"name\"]\n",
    "\n",
    "        feature_current = pd.Series(\"no\", index = data_test.index, name = feature_name)\n",
    "        feature_current[mask_current] = \"yes\"\n",
    "        data_test = pd.concat([data_test, feature_current], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6 - predictions on test data - preprocess data for machine learning ### ----\n",
    "\n",
    "# separate target variable Y from features X\n",
    "X3 = data1.drop([\"converted\"], axis = 1)\n",
    "Y3 = data1[\"converted\"]\n",
    "\n",
    "# keep only the 7 selected features\n",
    "selected_features3 = ['age_5', 'page_5', 'US_or_young', 'China_or_China', 'old_or_Direct', 'old_user_or_few', \n",
    "    'old_user_or_mid']\n",
    "X3 = X3[selected_features3]\n",
    "X_without_labels = data_test[selected_features3]\n",
    "\n",
    "# get features by category\n",
    "features_all3 = X3.columns\n",
    "features_num3 = X3._get_numeric_data().columns\n",
    "features_cat3 = features_all3.drop(features_num3)\n",
    "\n",
    "# create preprocessor object from pipelines for numeric and categorical features\n",
    "numeric_transformer3 = Pipeline(steps = [\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer3 = Pipeline(steps = [\n",
    "    (\"encoder\", OneHotEncoder(drop = \"first\", sparse_output = False))\n",
    "])\n",
    "preprocessor3 = ColumnTransformer(transformers = [\n",
    "    (\"num\", numeric_transformer3, features_num3),\n",
    "    (\"cat\", categorical_transformer3, features_cat3)\n",
    "])\n",
    "preprocessor3 = preprocessor3.set_output(transform = \"pandas\")\n",
    "\n",
    "# scale numeric features and encode categorical features\n",
    "X3 = preprocessor3.fit_transform(X3)\n",
    "X_without_labels = preprocessor3.transform(X_without_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6 - predictions on test data - train model and make predictions ### ----\n",
    "\n",
    "# train model\n",
    "classifier3 = LogisticRegression(C = gridsearch.best_params_[\"C\"])\n",
    "classifier3.fit(X3, Y3)\n",
    "\n",
    "# make predictions and dump to file\n",
    "data3 = {\n",
    "    'converted': classifier3.predict(X_without_labels)\n",
    "}\n",
    "Y_predictions = pd.DataFrame(columns = ['converted'], data = data3)\n",
    "Y_predictions.to_csv('conversion_data_test_predictions_CELINE-model1.csv', index = False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 7 - Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7 - feature importance ### ----\n",
    "\n",
    "# get column names from the preprocessor\n",
    "column_names3 = []\n",
    "for name, pipeline, features_list in preprocessor3.transformers_: \n",
    "    if name == 'num': \n",
    "        features = features_list \n",
    "    else: \n",
    "        features = pipeline.named_steps['encoder'].get_feature_names_out() \n",
    "    column_names3.extend(features)\n",
    "\n",
    "# store coefficients in a dataframe\n",
    "coefs3 = pd.DataFrame(index = range(0,len(classifier3.coef_[0,:])), columns = [\"features\", \"coefficients\"])\n",
    "coefs3[\"features\"] = column_names3\n",
    "coefs3[\"coefficients\"] = abs(classifier3.coef_[0,:])\n",
    "\n",
    "# get feature importance\n",
    "feature_importance3 = coefs3.sort_values(\"coefficients\", ascending = False).reset_index(drop = True)\n",
    "\n",
    "# plot feature importance\n",
    "fig3 = go.Figure([go.Bar(\n",
    "    x = feature_importance3[\"features\"],\n",
    "    y = feature_importance3[\"coefficients\"],\n",
    "    marker_color = px.colors.qualitative.Vivid)])\n",
    "\n",
    "# update layout\n",
    "fig3.update_xaxes(tickfont = dict(size = 10), tickangle = 90)\n",
    "fig3.update_yaxes(tickfont = dict(size = 10))\n",
    "fig3.update_layout(\n",
    "        margin = dict(l = 120),\n",
    "        title_text = \"Figure 3. Feature importance\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        xaxis = dict(title = \"Features\"),\n",
    "        yaxis = dict(title = \"Coefficients\", range = [-1, 21], tickvals = [0, 5, 10, 15, 20]),\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Projets_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
