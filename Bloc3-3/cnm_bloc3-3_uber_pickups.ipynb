{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloc 3 - Analyse prédictive de données structurées par l'intelligence artificielle - Uber pickups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Uber is a company founded in 2009, initially to provide ridesharing services. Since then, Uber expanded its activities to delivery (food, packages, couriers), freight transport and alternative means of urban transportation (bikes, scooters). Uber operates in about 70 countries and 10.500 cities and generates an average of 23 million trips per day.\n",
    "\n",
    "### Problematic\n",
    "\n",
    "Uber experiences ride cancellations because drivers are not always in close proximity with users.\n",
    "\n",
    "The company would like to recommend hot-zones to their drivers.\n",
    "\n",
    "### Scope\n",
    "\n",
    "Uber aims at creating an application that would recommend hot-zones to their drivers at any given time of the day. The uber team already has data about pickups in major cities. They would like to test this new feature by focusing on New York City.\n",
    "\n",
    "### Aim and objectives\n",
    "\n",
    "Overall aim: Identify pickup hot-zones in New York City.\n",
    "\n",
    "Objectives:\n",
    "- 1 - Find hot-zones (compare two unsupervised algorithms).\n",
    "- 2 - Visualize results on a dashboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Methods\n",
    "\n",
    "### 1 - Library import\n",
    "\n",
    "### 2 - File reading and basic exploration\n",
    "\n",
    "The dataset was composed of 8 files containing information on taxis and uber pickups during the years 2014 and 2015. For the purpose of this project, only information about Uber pickups for the months of April, May, June, and September 2014 were used. Data from July and August 2014 were excluded not to introduce a bias due to summer holidays. Data from 2015 was excluded due to a lack of geographic coordinates.\n",
    "\n",
    "The resulting dataset contained information on 2.908.931 pickups, including geophaphic coordinates, date, and time. It did not contain any missing values.\n",
    "\n",
    "### 3 - Preprocessing and exploratory data analysis\n",
    "\n",
    "Before performing an exploratory data analysis, two features were created, for the day of the week and the hour of the pickups.\n",
    "\n",
    "The pickup time distribution was displayed for each day of the week, as well as a map summarizing the locations of the pickups. It appeared that most rides occur at the end of each day, and that pickup locations could be narrowed down around Manhattan.\n",
    "\n",
    "### 4 - Data selection\n",
    "\n",
    "First, pickups considered as outliers (with latitude or longitude further away than four standard deviations from their respective means) were dropped.\n",
    "\n",
    "Then, data was selected according to pickup peak hours, when the demand from users is the most important. Indeed, it was assumed that a high demand could cause a drop in drivers' availability, and therefore an increase in waiting time. Thus it was more relevant to identify hot-zones for peak hours to respond to a maximum of demands. To identify peak hours, hours were sorted by pickup probability, and a cut-off based on cumulative probabilities was applied to select peak hours.\n",
    "\n",
    "### 5 - Clustering with K-Means\n",
    "\n",
    "The optimal k number of clusters was determined for each day of the week with the Elbow method. To identify the maximum number of relevant clusters, the highest k leading to more than a 5% decrease of the within cluster sum of square was selected.\n",
    "\n",
    "K-Means clustering was then performed for each day of the week with the corresponding k and the default k-means++ method for initialization of the clusters.\n",
    "\n",
    "### 6 - Clustering with DBSCAN\n",
    "\n",
    "Clustering was also performed on the same data with DBSCAN to compare the two clustering methods. Since the data consisted in geographic coordinates, and since we awere dealing with car rides in right-angle organized streets, the Manhattan distance was chosen as metric to calculate distances between pickups. The maximum distance between pickups to be considered as neighboors (epsilon) was selected empirically to be identical for all days of the week. The minimum number of neighboors for a point to be considered as a core point was set to 1/500 of the total number of pickups for each day to account for differences in pickup density between days.\n",
    "\n",
    "### 7 - Dashboard\n",
    "\n",
    "For each day of the week, hot-zones during peak hours were displayed on the New York City map. Results obtained with the two methods were plotted in parallel for comparison.\n",
    "\n",
    "### 8 - Focus on a Sunday in Manhattan\n",
    "\n",
    "As a proof of concept, clustering was performed with DBSCAN to depict hot-zones per hour for Sundays in Manhattan."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Conclusion\n",
    "\n",
    "The dataset of Uber, covering almost 3.000.000 pickups, allowed for the identification of hot-zones at peak hours to better respond to the demand of users, and therefore decrease the number of ride cancellations.\n",
    "\n",
    "As expected for this type of data where some clusters are elongated (Manhattan district for example), and where clusters have different shapes and sizes, DBSCAN performed better than K-Means in defining hot-zones. These hot-zones were globally the same across days. They included the Manhattan district (with a very high demand all over the district), the three airports (J.F. Kennedy International Airport, Newark Liberty International Airport, and LaGuardia Airport), and areas in Brooklyn such as Williamsburg (trendy neighboorhood also called the \"Little Berlin\"), which are close to the bridges to Manhattan. Amusing enough, a small cluster appeared during the week-end right at the Ikea store.\n",
    "\n",
    "Because of limitations regarding the display of maps, the analysis was limited to peak hours and days of the week. However, the dashboard could easily be augmented with hot-zones depicted per hour for each day. Additionnaly, a map of hot-zones dedicated to the Manhattan district could be very insightful. A proof of concept for Sundays in Manhattan showed this approach to be feasible. Despite current limitations, Uber could already ask their drivers to be more present in the identified hot-zones at peak hours."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 - library import ### ----\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 2 - File reading and basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - import dataset ### ----\n",
    "\n",
    "# load data\n",
    "data_uber_apr14 = pd.read_csv(\"cnm_bloc3-3_uber-raw-data-apr14.csv\")\n",
    "data_uber_may14 = pd.read_csv(\"cnm_bloc3-3_uber-raw-data-may14.csv\")\n",
    "data_uber_jun14 = pd.read_csv(\"cnm_bloc3-3_uber-raw-data-jun14.csv\")\n",
    "data_uber_sep14 = pd.read_csv(\"cnm_bloc3-3_uber-raw-data-sep14.csv\")\n",
    "\n",
    "# excluded data\n",
    "#data_taxi = pd.read_csv(\"cnm_bloc3-3_taxi-zone-lookup.csv\")\n",
    "#data_uber_jul14 = pd.read_csv(\"cnm_bloc3-3_uber-raw-data-jul14.csv\")\n",
    "#data_uber_aug14 = pd.read_csv(\"cnm_bloc3-3_uber-raw-data-aug14.csv\")\n",
    "#data_uber_janjune15 = pd.read_csv(\"cnm_bloc3-3_uber-raw-data-janjune-15.csv\")\n",
    "\n",
    "# concatenate selected data\n",
    "data = pd.concat([data_uber_apr14, data_uber_may14, data_uber_jun14, data_uber_sep14], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get basic stats ### ----\n",
    "\n",
    "# print shape of data\n",
    "print(\"Number of rows: {}\".format(data.shape[0]))\n",
    "print(\"Number of columns: {}\".format(data.shape[1]))\n",
    "print()\n",
    "\n",
    "# display dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Dataset display: \")\n",
    "display(data.head())\n",
    "print()\n",
    "\n",
    "# display basic statistics\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = data.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n",
    "\n",
    "# display percentage of missing values in columns and rows\n",
    "percent_nan_col = data.isnull().sum() / data.shape[0] * 100\n",
    "print(\"Percentage of missing values per column:\\n{}\".format(percent_nan_col))\n",
    "print()\n",
    "percent_nan_row = data[data.isnull().all(axis = 1)].shape[0] / data.shape[1] * 100\n",
    "print(\"Percentage of rows fully filled with missing values: {}\".format(percent_nan_row))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 3 - Preprocessing and exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing and exploratory data analysis - create features from date ### ----\n",
    "\n",
    "# copy data for safety\n",
    "data1 = data.copy()\n",
    "\n",
    "# create usable features from the date column\n",
    "data1[\"Date/Time\"] = pd.to_datetime(data1[\"Date/Time\"], infer_datetime_format = True)\n",
    "data1[\"day_of_week\"] = data1[\"Date/Time\"].dt.day_of_week\n",
    "data1[\"hour\"] = data1[\"Date/Time\"].dt.hour\n",
    "\n",
    "# drop useless columns\n",
    "data1 = data1.drop([\"Base\", \"Date/Time\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - preprocessing and exploratory data analysis - distributions and map ### ----\n",
    "\n",
    "# get unique days of week\n",
    "days_unique = np.sort(data1[\"day_of_week\"].unique())\n",
    "\n",
    "# set figure to make subplots\n",
    "fig1 = make_subplots(\n",
    "    rows = 2,\n",
    "    cols = 4,\n",
    "    specs = [[{}, {}, {}, {}], [{}, {}, {}, {'type': 'mapbox'}]],\n",
    "    subplot_titles = (\n",
    "        \"Monday\",\n",
    "        \"Tuesday\",\n",
    "        \"Wednesday\",\n",
    "        \"Thursday\",\n",
    "        \"Friday\",\n",
    "        \"Saturday\",\n",
    "        \"Sunday\",\n",
    "        \"All days\"),\n",
    "    horizontal_spacing = 0.10,\n",
    "    vertical_spacing = 0.30)\n",
    "\n",
    "# plot distribution of pickups per day of the week\n",
    "[fig1.add_trace(go.Histogram(\n",
    "    x = data1.loc[data1[\"day_of_week\"] == days_unique[i],\"hour\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[7],\n",
    "    showlegend = False), \n",
    "    row = 1, col = i+1) for i in [0, 1, 2, 3]]\n",
    "[fig1.add_trace(go.Histogram(\n",
    "    x = data1.loc[data1[\"day_of_week\"] == days_unique[i],\"hour\"],\n",
    "    marker_color = px.colors.qualitative.Vivid[7],\n",
    "    showlegend = False), \n",
    "    row = 2, col = i-3) for i in [4, 5, 6]]\n",
    "\n",
    "# plot map\n",
    "data_fig1 = data1.sample(5000, random_state = 0)\n",
    "fig1.add_trace(go.Scattermapbox(\n",
    "        lat = data_fig1[\"Lat\"], \n",
    "        lon = data_fig1[\"Lon\"],\n",
    "        marker_color = px.colors.qualitative.Vivid[7],\n",
    "        marker_size = 2),\n",
    "        row = 2, col = 4)\n",
    "\n",
    "# update layout\n",
    "fig1.update_xaxes(title = \"Hour\", tickfont = dict(size = 10))\n",
    "fig1.update_yaxes(tickfont = dict(size = 10), range = [-1000, 48000])\n",
    "fig1.update_layout(\n",
    "        margin = dict(l = 90),\n",
    "        title_text = \"Figure 1. Pickup time distribution\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 500)\n",
    "fig1.update_mapboxes(\n",
    "    style = \"carto-positron\",\n",
    "    center = dict(\n",
    "        lat = data1.loc[:,\"Lat\"].mean(),\n",
    "        lon = data1.loc[:,\"Lon\"].mean()),\n",
    "    zoom = 7.5)\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 4 - Data selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - data selection - narrow down around Manhattan ### ----\n",
    "\n",
    "# consider as outliers pickups whose latitude or longitude is more than four standard deviations \n",
    "# away from the mean\n",
    "\n",
    "# copy data for safety\n",
    "data2 = data1.copy()\n",
    "\n",
    "# set columns to check\n",
    "columns_check = [\"Lat\",\"Lon\"]\n",
    "\n",
    "# initialise count for droped rows\n",
    "drop_count = 0\n",
    "\n",
    "# loop through columns\n",
    "for column in columns_check:\n",
    "\n",
    "    # get lower and upper bonds\n",
    "    bond_lower = data2[column].mean() - 4 * data2[column].std()\n",
    "    bond_upper = data2[column].mean() + 4 * data2[column].std()\n",
    "\n",
    "    # get index of rows to drop\n",
    "    drop_mask = (data2[column] < bond_lower) | (data2[column] > bond_upper)\n",
    "    drop_index = data2.loc[drop_mask,:].index\n",
    "    drop_count += len(drop_index)\n",
    "\n",
    "    # drop rows containing outliers\n",
    "    data2 = data2.drop(drop_index, axis = 0)\n",
    "\n",
    "# print number of rows that were droped\n",
    "print(\"Number of rows that were droped: {}\".format(drop_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - data selection - narrow down to peak hours ### ----\n",
    "\n",
    "# get unique days of week\n",
    "days_unique = np.sort(data2[\"day_of_week\"].unique())\n",
    "\n",
    "# initialise variable to downsampled data\n",
    "data_sample = pd.DataFrame(columns = [\"Lat\",\"Lon\",\"day_of_week\"])\n",
    "\n",
    "# loop through days\n",
    "for day in days_unique:\n",
    "\n",
    "    # get data for the current day\n",
    "    data_current = data2.loc[data2[\"day_of_week\"] == day,:]\n",
    "\n",
    "    # sort hours by pickup probability\n",
    "    pickup_proba = pd.DataFrame((data_current[\"hour\"].value_counts() / \\\n",
    "        data_current.shape[0]).sort_values(ascending = False)).reset_index()\n",
    "    pickup_proba.columns = [\"hour\",\"proba\"]\n",
    "\n",
    "    # select peak hours (cumulative probabilities < 50%)\n",
    "    pickup_proba[\"proba_cum\"] = np.cumsum(pickup_proba[\"proba\"])\n",
    "    peak_hours = pickup_proba.loc[pickup_proba[\"proba_cum\"] < 0.5,\"hour\"].values\n",
    "    data_current = data_current.loc[data_current[\"hour\"].isin(peak_hours),:]\n",
    "\n",
    "    # store data\n",
    "    data_sample = pd.concat([data_sample, data_current.loc[:,[\"Lat\",\"Lon\",\"day_of_week\"]]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 5 - Clustering with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - clustering with k-means - get optimal k with elbow ### ----\n",
    "\n",
    "# get optimal number of clusters per day of the week\n",
    "\n",
    "# initialise variable to store number of clusters\n",
    "cluster_nb = pd.DataFrame(index = range(0,len(days_unique)), columns = [\"day\", \"cluster_nb\"])\n",
    "cluster_nb[\"day\"] = days_unique\n",
    "\n",
    "# loop through days\n",
    "for day in days_unique:\n",
    "\n",
    "    # set mask for day\n",
    "    mask_day = data_sample[\"day_of_week\"] == day\n",
    "\n",
    "    # get current data\n",
    "    data_current = data_sample.loc[mask_day,[\"Lat\",\"Lon\"]]\n",
    "\n",
    "    # initialise temporary variable to store within sum of square\n",
    "    wcss_temp = []\n",
    "    wcss =  pd.DataFrame(index = range(0,10), columns = [\"k\",\"wcss\",\"wcss_norm\",\"diff\"])\n",
    "    wcss[\"k\"] = range(1,11)\n",
    "\n",
    "    # get within cluster sum of square for k ranging from 1 to 15\n",
    "    for k in wcss[\"k\"]: \n",
    "        kmeans = KMeans(n_clusters = k, random_state = 0, n_init = \"auto\")\n",
    "        kmeans.fit(data_current)\n",
    "        wcss_temp.append(kmeans.inertia_)\n",
    "    \n",
    "    # normalize wcss and select best k\n",
    "    wcss[\"wcss\"] = wcss_temp\n",
    "    wcss[\"wcss_norm\"] = wcss[\"wcss\"] * 100 / wcss.loc[0,\"wcss\"]\n",
    "    wcss.loc[1:,\"diff\"] = wcss.loc[0:8,\"wcss_norm\"].values - wcss.loc[1:,\"wcss_norm\"].values\n",
    "    best_k = wcss.loc[wcss[\"diff\"] > 5,\"k\"].max()\n",
    "    cluster_nb.loc[cluster_nb[\"day\"] == day,\"cluster_nb\"] = best_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - clustering with k-means - cluster with k-means ### ----\n",
    "\n",
    "# initialise variable to store clusters and cluster center coordinates\n",
    "kmeans_data = pd.DataFrame(columns = [\"Lat\",\"Lon\",\"day_of_week\",\"cluster\",\"cluster_weight\",\"weight_sort\"])\n",
    "\n",
    "# loop through days\n",
    "for day in days_unique:\n",
    "\n",
    "    # set mask for day\n",
    "    mask_day = data_sample[\"day_of_week\"] == day\n",
    "\n",
    "    # get current data\n",
    "    data_current = data_sample.loc[mask_day,[\"Lat\",\"Lon\",\"day_of_week\"]]\n",
    "    cluster_nb_current = cluster_nb.loc[cluster_nb[\"day\"] == day,\"cluster_nb\"].values[0]\n",
    "\n",
    "    # fit k-means \n",
    "    kmeans = KMeans(n_clusters = cluster_nb_current, random_state = 0, n_init = \"auto\")\n",
    "    kmeans.fit(data_current.loc[:,[\"Lat\",\"Lon\"]])\n",
    "\n",
    "    # get clusters\n",
    "    data_current = data_current.assign(cluster = kmeans.predict(data_current.loc[:,[\"Lat\",\"Lon\"]]))\n",
    "\n",
    "    # add cluster weight for plotting\n",
    "    cluster_weights = data_current[\"cluster\"].value_counts()\n",
    "    weights = [cluster_weights[cluster] for cluster in data_current[\"cluster\"]] \n",
    "    data_current[\"cluster_weight\"] = weights\n",
    "    weight_sort = cluster_weights.sort_values(ascending = False).reset_index(drop = True)\n",
    "    data_current[\"weight_sort\"] = [weight_sort[weight_sort.values == weight].index[0]\n",
    "        for weight in data_current[\"cluster_weight\"]]\n",
    "\n",
    "    # concatenate\n",
    "    kmeans_data = pd.concat([kmeans_data, data_current], ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 6 - Clustering with DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6 - clustering with dbscan ### ----\n",
    "\n",
    "# initialise variable to store clusters and cluster center coordinates\n",
    "dbscan_data = pd.DataFrame(columns = [\"Lat\",\"Lon\",\"day_of_week\",\"cluster\",\"cluster_weight\",\"weight_sort\"])\n",
    "\n",
    "# loop through days\n",
    "for day in days_unique:\n",
    "\n",
    "    # set mask for day\n",
    "    mask_day = data_sample[\"day_of_week\"] == day\n",
    "\n",
    "    # get current data\n",
    "    data_current = data_sample.loc[mask_day,:]\n",
    "\n",
    "    # fit dbscan\n",
    "    dbscan = DBSCAN(eps = 0.005, min_samples = np.round(data_current.shape[0] / 500).astype(int), \n",
    "        metric = \"manhattan\", n_jobs = -2)\n",
    "    dbscan.fit(data_current.loc[:,[\"Lat\",\"Lon\"]])\n",
    "\n",
    "    # get clusters\n",
    "    data_current = data_current.assign(cluster = dbscan.labels_)\n",
    "\n",
    "    # add cluster weight for plotting\n",
    "    cluster_weights = data_current[\"cluster\"].value_counts() * 100 / data_current.shape[0]\n",
    "    weights = [cluster_weights[cluster] for cluster in data_current[\"cluster\"]] \n",
    "    data_current[\"cluster_weight\"] = weights\n",
    "    weight_sort = cluster_weights.sort_values(ascending = False).reset_index(drop = True)\n",
    "    data_current[\"weight_sort\"] = [weight_sort[weight_sort.values == weight].index[0]\n",
    "        for weight in data_current[\"cluster_weight\"]]\n",
    "\n",
    "    # concatenate\n",
    "    dbscan_data = pd.concat([dbscan_data, data_current], ignore_index = True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 7 - Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7 - dashboard ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig2 = make_subplots(\n",
    "    rows = 7,\n",
    "    cols = 2,\n",
    "    specs = [[{'type': 'mapbox'}, {'type': 'mapbox'}], [{'type': 'mapbox'}, {'type': 'mapbox'}], \n",
    "        [{'type': 'mapbox'}, {'type': 'mapbox'}], [{'type': 'mapbox'}, {'type': 'mapbox'}], \n",
    "        [{'type': 'mapbox'}, {'type': 'mapbox'}], [{'type': 'mapbox'}, {'type': 'mapbox'}], \n",
    "        [{'type': 'mapbox'}, {'type': 'mapbox'}]],\n",
    "    subplot_titles = (\n",
    "        \"K-Means\", \"DBSCAN\"),\n",
    "    column_widths = [0.40, 0.40],\n",
    "    horizontal_spacing = 0.15,\n",
    "    vertical_spacing = 0.03)\n",
    "\n",
    "# plot k-means clusters\n",
    "for i in range(0,7):\n",
    "    data_current = kmeans_data.loc[kmeans_data[\"day_of_week\"] == days_unique[i],:].sample(n = 20000,\n",
    "        random_state = 0)\n",
    "    fig2.add_trace(go.Scattermapbox(\n",
    "        lat = data_current[\"Lat\"], \n",
    "        lon = data_current[\"Lon\"],\n",
    "        marker_colorscale = \"Portland\",\n",
    "        marker_color = data_current[\"weight_sort\"],\n",
    "        marker_size = 3),\n",
    "        row = i+1, col = 1)\n",
    "\n",
    "# plot dbscan clusters (without outliers)\n",
    "for i in range(0,7):\n",
    "    data_current = dbscan_data.loc[dbscan_data[\"day_of_week\"] == days_unique[i],:].sample(n = 20000,\n",
    "        random_state = 0)\n",
    "    drop_index = data_current.loc[data_current[\"cluster\"] == -1,:].index\n",
    "    data_current = data_current.drop(drop_index, axis = 0)\n",
    "    fig2.add_trace(go.Scattermapbox(\n",
    "        lat = data_current[\"Lat\"], \n",
    "        lon = data_current[\"Lon\"],\n",
    "        marker_colorscale = \"Portland\",\n",
    "        marker_color = data_current[\"weight_sort\"],\n",
    "        marker_size = 3),\n",
    "        row = i+1, col = 2)\n",
    "\n",
    "# add day annotations\n",
    "fig2.add_annotation(text = \"Monday\", xref = \"paper\", yref = \"paper\", x = -0.07, y = 0.96, textangle = -90, \n",
    "    showarrow = False)    \n",
    "fig2.add_annotation(text = \"Tuesday\", xref = \"paper\", yref = \"paper\", x = -0.07, y = 0.81, textangle = -90, \n",
    "    showarrow = False)    \n",
    "fig2.add_annotation(text = \"Wednesday\", xref = \"paper\", yref = \"paper\", x = -0.07, y = 0.67, textangle = -90, \n",
    "    showarrow = False)    \n",
    "fig2.add_annotation(text = \"Thursday\", xref = \"paper\", yref = \"paper\", x = -0.07, y = 0.50, textangle = -90, \n",
    "    showarrow = False)    \n",
    "fig2.add_annotation(text = \"Friday\", xref = \"paper\", yref = \"paper\", x = -0.07, y = 0.35, textangle = -90, \n",
    "    showarrow = False)    \n",
    "fig2.add_annotation(text = \"Saturday\", xref = \"paper\", yref = \"paper\", x = -0.07, y = 0.19, textangle = -90, \n",
    "    showarrow = False)    \n",
    "fig2.add_annotation(text = \"Sunday\", xref = \"paper\", yref = \"paper\", x = -0.07, y = 0.04, textangle = -90, \n",
    "    showarrow = False)    \n",
    "fig2.update_annotations(font_size = 15)\n",
    "\n",
    "# update layout\n",
    "fig2.update_layout(\n",
    "        margin = dict(l = 90, t= 120),\n",
    "        title_text = \"Figure 2. Hot-zones per week day\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.98,\n",
    "        title_font_size = 18,\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 2000)\n",
    "fig2.update_mapboxes(\n",
    "    style=\"carto-positron\",\n",
    "    center = dict(\n",
    "        lat = data2.loc[:,\"Lat\"].mean(),\n",
    "        lon = data2.loc[:,\"Lon\"].mean()),\n",
    "    zoom = 8.5)\n",
    "\n",
    "fig2.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 8 - Focus on a Sunday in Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8 - focus on a sunday in manhattan - process data ### ----\n",
    "\n",
    "# copy data for safety\n",
    "data3 = data1.copy()\n",
    "\n",
    "# part 1 - select data\n",
    "\n",
    "# set geographic boundaries for manhattan\n",
    "bound_south = 40.70\n",
    "bound_north = 40.87\n",
    "bound_east = -73.92\n",
    "bound_west = -74.02\n",
    "\n",
    "# set mask and select data to keep\n",
    "mask_keep = (data3[\"Lat\"] > bound_south) & (data3[\"Lat\"] < bound_north) & (data3[\"Lon\"] < bound_east) & \\\n",
    "    (data3[\"Lon\"] > bound_west) & (data3[\"day_of_week\"] == 6)\n",
    "data3 = data3.loc[mask_keep,:]\n",
    "\n",
    "\n",
    "# part 2 - cluster with dbscan\n",
    "\n",
    "# initialise variable to store clusters and cluster center coordinates\n",
    "manhattan_data = pd.DataFrame(columns = [\"Lat\",\"Lon\",\"hour\",\"cluster\"])\n",
    "\n",
    "# get unique hour data\n",
    "hours_unique = data3[\"hour\"].unique()\n",
    "\n",
    "# loop through hours\n",
    "for hour in hours_unique:\n",
    "\n",
    "    # set mask for hour\n",
    "    mask_hour = data3[\"hour\"] == hour\n",
    "\n",
    "    # get current data\n",
    "    data_current = data3.loc[mask_hour,[\"Lat\",\"Lon\",\"hour\"]]\n",
    "\n",
    "    # fit dbscan\n",
    "    dbscan = DBSCAN(eps = 0.0012, min_samples = np.round(data_current.shape[0] / 200).astype(int), \n",
    "        metric = \"manhattan\", n_jobs = -2)\n",
    "    dbscan.fit(data_current.loc[:,[\"Lat\",\"Lon\"]])\n",
    "\n",
    "    # get clusters\n",
    "    data_current = data_current.assign(cluster = dbscan.labels_)\n",
    "\n",
    "    # concatenate\n",
    "    manhattan_data = pd.concat([manhattan_data, data_current], ignore_index = True)\n",
    "\n",
    "# drop outliers for plotting\n",
    "manhattan_data = manhattan_data.loc[manhattan_data[\"cluster\"] != -1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8 - focus on a sunday in manhattan - plot clusters on map ### ----\n",
    "\n",
    "# plot dbscan clusters (without outliers)\n",
    "fig3 = px.scatter_mapbox(\n",
    "    manhattan_data, \n",
    "    lat = \"Lat\", \n",
    "    lon = \"Lon\", \n",
    "    color = \"cluster\",\n",
    "    animation_frame = \"hour\")\n",
    "\n",
    "fig3[\"layout\"].pop(\"updatemenus\") \n",
    "\n",
    "# update layout\n",
    "fig3.update_layout(\n",
    "        title_text = \"Figure 3. Hot-zones in Manhattan on Sundays\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 600)\n",
    "fig3.update_mapboxes(\n",
    "    style=\"carto-positron\",\n",
    "    center = dict(\n",
    "        lat = bound_north - (bound_north - bound_south) / 2,\n",
    "        lon = bound_east - (bound_east - bound_west) / 2),\n",
    "    zoom = 10)\n",
    "\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6274bd53ad02975ad3a1fc7c53468a83b5e714d6ade680c552f31a0436ec5d39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
