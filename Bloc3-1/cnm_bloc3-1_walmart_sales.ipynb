{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloc 3 - Analyse prédictive de données structurées par l'intelligence artificielle - Walmart sales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Walmart Inc. is an American multinational retail corporation founded in 1962. As of 2022, Walmart had over 10.000 stores and clubs in 24 countries, operating a chain of hypermarkets, discount department stores, and grocery stores. In 2022, Walmart was rated the world's largest company by revenue, and the world's largest private employer.\n",
    "\n",
    "### Problematic\n",
    "\n",
    "Walmart aims at planning and improving its future marketing campaigns.\n",
    "\n",
    "The company would like to better understand how the sales are influenced by economic indicators.\n",
    "\n",
    "### Scope\n",
    "\n",
    "To predict the amount of sales from economic indicators, Walmart provided a dataset containing information about weekly sales for different Walmart stores, as well as other variables such as the unemployment rate or the fuel price.\n",
    "\n",
    "### Aim and objectives\n",
    "\n",
    "Overall aim: Build a machine learning model able to estimate the weekly sales in Walmart stores.\n",
    "\n",
    "Objectives:\n",
    "- 1 - Make an EDA and preprocess data for machine learning.\n",
    "- 2 - Train a linear regression model (baseline) and assess its performance.\n",
    "- 3 - Identify features that are important for the prediction.\n",
    "- 4 - Avoid overfitting by training a regularized regression model.\n",
    "- 5 - Fine-tune the regularized regression model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Methods\n",
    "\n",
    "### 1 - Library import\n",
    "\n",
    "### 2 - File reading and basic exploration\n",
    "\n",
    "The dataset was composed of 150 records of weekly sales for 20 stores. It contained 8 features: the store identification number, the date of the record, the weekly sales, a flag for holidays, the temperature, the fuel price, the consumer price index, and the unemployment rate. \n",
    "\n",
    "The initial inspection of the dataset revealed that all features but the store id contained missing values, that the \"Date\" column could be split to obtain additional features, and that some variables might contain outliers.\n",
    "\n",
    "### 3 - Exploratory data analysis\n",
    "\n",
    "An univariate analysis, a bivariate analysis and a correlation matrix were plotted to obtain information on the distribution of each variable, possible correlations and interdependencies between features.\n",
    "\n",
    "### 4 - Prepocessing\n",
    "\n",
    "According to what was observed, a first preprocessing was performed with the following steps:\n",
    "- 1 - Rows containing missing values in the target variable \"Weekly_Sales\" or in the variables \"Date\" and \"Holiday_Flag\" were dropped.\n",
    "- 2 - Columns recording the year, the month, the day, and the day of the week were created from the \"Date\" column, which was subsequently dropped.\n",
    "- 3 - Rows containing outliers (defined as values diverging from the mean by more than three standard deviations) in the features \"Temperature\", \"Fuel_Price\", \"CPI\", and \"Unemployment\" were dropped.\n",
    "\n",
    "Data was then processed for machine learning:\n",
    "- 1 - The target variable was separated from the features.\n",
    "- 2 - The dataset was divided into train and test sets.\n",
    "- 3 - For numeric features, missing values were imputed and scaled. Categorical features were encoded.\n",
    "\n",
    "### 5 - Baseline model (linear regression)\n",
    "\n",
    "A linear regression model was trained on the train set and its performances were assessed.\n",
    "\n",
    "### 6 - Baseline model feature importance\n",
    "\n",
    "The feature importance of the baseline model was displayed from the model coefficients.\n",
    "\n",
    "### 7 - Regularized regression model (Ridge)\n",
    "\n",
    "A regularized linear regression model (Ridge) was trained on the train set and its performances were assessed.\n",
    "\n",
    "### 8 - Regularized model feature importance\n",
    "\n",
    "The feature importance of the regularized model was displayed from the model coefficients.\n",
    "\n",
    "### 9 - Regularized model fine-tuning\n",
    "\n",
    "The regularization strength of the regularized model was fine-tuned. The fine-tuned model was trained on the train set and its performances were assessed.\n",
    "\n",
    "### 10 - Regularized model feature importance after tuning\n",
    "\n",
    "The feature importance of the fine-tuned regularized model was displayed from the model coefficients.\n",
    "\n",
    "### 11 - Results summary\n",
    "\n",
    "The performance indicators (cross-validated R2, R2 standard deviation, R2 train, R2 test) obtained from the three models were displayed for comparison.\n",
    "\n",
    "### 12 - Store selection and economic indicators\n",
    "\n",
    "To investigate whether the selection of stores made by Walmart was the best for the prediction of weekly sales, data from the 6 top stores (having coefficients above 1M for the third model) was extracted and a correlation matrix of the economics indicators was drawned to gain insights on possible correlations with the weekly sales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Conclusion\n",
    "\n",
    "The dataset provided by Walmart was small (109 rows after cleaning) but allowed for the production of predictive models and for the identification of the best features for predictions.\n",
    "\n",
    "The first model (linear regression) already gave good performances, with a R2 of 0.98 on the train set and a R2 of 0.93 on the test set. Unexpectedly, no major overfitting was observed for this model. The second model (Ridge with default lambda equal to 1.0) performed less well, but was improved by the fine-tuning of its regularization strength to a value of 0.01. The resulting R2s on the train and test sets were comparable to the ones obtained with the linear regression model.\n",
    "\n",
    "Regarding the feature importance, it seemed that for all tested models, stores themselves were the most important factors for the prediction, while economic indicators did not influence much the predictions of weekly sales.\n",
    "\n",
    "For their future marketing campaigns, it would be advisable for the marketing service of Walmart to focus on the selection of the stores they use for their study if they aim at using economic indicators. As an example, the selection of the six most predictive stores revealed a strong negative correlation between the CPI and the weekly sales (coefficient = -0.72) which could be used in the future."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "## Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1 - library import ### ----\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import  OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 2 - File reading and basic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - import dataset ### ----\n",
    "\n",
    "# read data\n",
    "data = pd.read_csv(\"cnm_bloc3-1_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get basic stats ### ----\n",
    "\n",
    "# print shape of data\n",
    "print(\"Number of rows: {}\".format(data.shape[0]))\n",
    "print(\"Number of columns: {}\".format(data.shape[1]))\n",
    "print()\n",
    "\n",
    "# display dataset\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"Dataset display: \")\n",
    "display(data.head())\n",
    "print()\n",
    "\n",
    "# display basic statistics\n",
    "print(\"Basics statistics: \")\n",
    "data_desc = data.describe(include='all')\n",
    "display(data_desc)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2 - file reading and basic exploration - get percentage of missing values ### ----\n",
    "\n",
    "# check wether some columns are full of NaNs\n",
    "column_nan_full = data.columns[data.isnull().all()]\n",
    "column_nb = len(column_nan_full)\n",
    "\n",
    "# get percentage of missing values in columns\n",
    "percent_nan_col = data.isnull().sum() / data.shape[0] * 100\n",
    "\n",
    "# check wether some rows are full of NaNs\n",
    "row_nan_count = pd.Series([data.loc[i,:].isnull().sum() for i in range(0, data.shape[0])])\n",
    "row_nan_full = row_nan_count.index[row_nan_count == data.shape[1]]\n",
    "row_nb = len(row_nan_full)\n",
    "\n",
    "# print report\n",
    "print(\"COLUMNS\")\n",
    "print(\"{} columns out of {} are fully filled with missing values\".format(column_nb,data.shape[1]))\n",
    "print(\"Percentage of missing values per column:\\n{}\".format(percent_nan_col))\n",
    "print()\n",
    "print(\"ROWS\")\n",
    "print(\"{} rows out of {} are fully filled with missing values\".format(row_nb,data.shape[0]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 3 - Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - exploratory data analysis - plot univariate analysis ### ----\n",
    "\n",
    "# set figure to make subplots\n",
    "fig1 = make_subplots(\n",
    "    rows = 2,\n",
    "    cols = 4,\n",
    "    subplot_titles = (\n",
    "        \"A. Weekly sales\",\n",
    "        \"B. Temperature\",\n",
    "        \"C. Fuel price\",\n",
    "        \"D. CPI\",\n",
    "        \"E. Unemployment rate\",\n",
    "        \"F. Holidays\",\n",
    "        \"G. Stores\"),\n",
    "    column_widths = [0.20, 0.20, 0.20, 0.20],\n",
    "    horizontal_spacing = 0.15)\n",
    "\n",
    "# plot distribution of each numeric variable\n",
    "features_num = [\"Weekly_Sales\", \"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\"]\n",
    "[fig1.add_trace(go.Histogram(\n",
    "    x = data[features_num[i]],\n",
    "    marker_color = px.colors.qualitative.Vivid[i]),\n",
    "    row = 1, col = i+1) for i in [0, 1, 2, 3]]\n",
    "[fig1.add_trace(go.Histogram(\n",
    "    x = data[features_num[i]],\n",
    "    marker_color = px.colors.qualitative.Vivid[i]),\n",
    "    row = 2, col = i-3) for i in [4]]\n",
    "\n",
    "# plot categorical variables\n",
    "holidays = data[\"Holiday_Flag\"].value_counts()\n",
    "fig1.add_trace(go.Bar(\n",
    "    x = [\"No\", \"Yes\"],\n",
    "    y = holidays.values,\n",
    "    marker_color = px.colors.qualitative.Vivid[6:]),\n",
    "    row = 2, col = 2)\n",
    "stores = data[\"Store\"].value_counts()\n",
    "fig1.add_trace(go.Bar(\n",
    "    x = stores.index,\n",
    "    y = stores.values,\n",
    "    marker_color = px.colors.qualitative.Vivid[8]),\n",
    "    row = 2, col = 3)\n",
    "\n",
    "# update layout\n",
    "fig1.update_annotations(font_size = 15)\n",
    "fig1.update_xaxes(tickfont = dict(size = 10))\n",
    "fig1.update_yaxes(tickfont = dict(size = 10))\n",
    "fig1.update_layout(\n",
    "        margin = dict(l = 90, t= 120),\n",
    "        title_text = \"Figure 1. Univariate analysis\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        xaxis = dict(title = go.layout.xaxis.Title(text = \"Weekly sales (Dollars)\", font_size = 10)),\n",
    "        xaxis2 = dict(title = go.layout.xaxis.Title(text = \"Temperature (Degrees Fahrenheit)\", font_size = 10)),\n",
    "        xaxis3 = dict(title = go.layout.xaxis.Title(text = \"Fuel price (Dollars)\", font_size = 10)),\n",
    "        xaxis4 = dict(title = go.layout.xaxis.Title(text = \"CPI (AU)\", font_size = 10)),\n",
    "        xaxis5 = dict(title = go.layout.xaxis.Title(text = \"Unemployment rate (Percent)\", font_size = 10)),\n",
    "        xaxis6 = dict(title = go.layout.xaxis.Title(text = \"Holiday period\", font_size = 10)),\n",
    "        xaxis7 = dict(title = go.layout.xaxis.Title(text = \"Store id\", font_size = 10)),\n",
    "        yaxis = dict(range = [0, 24], tickvals = [0, 5, 10, 15, 20]),\n",
    "        yaxis2 = dict(range = [0, 24], tickvals = [0, 5, 10, 15, 20]),\n",
    "        yaxis3 = dict(range = [0, 36], tickvals = [0, 10, 20, 30]),\n",
    "        yaxis4 = dict(range = [0, 72], tickvals = [0, 20, 40, 60]),\n",
    "        yaxis5 = dict(range = [0, 30], tickvals = [0, 5, 10, 15, 20, 25]),\n",
    "        yaxis6 = dict(title = go.layout.yaxis.Title(text = \"Count\", font_size = 10, standoff = 0), \n",
    "            range = [0, 180], tickvals = [0, 50, 100, 150]),\n",
    "        yaxis7 = dict(title = go.layout.yaxis.Title(text = \"Count\", font_size = 10, standoff = 0), \n",
    "            range = [0, 18], tickvals = [0, 5, 10, 15]),\n",
    "        bargroupgap = 0.4,\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 600)\n",
    "\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - exploratory data analysis - plot bivariate analysis ### ----\n",
    "\n",
    "# plot pairwise dependencies\n",
    "fig2 = go.Figure(data = go.Splom(\n",
    "        dimensions = [dict(label = \"Weekly sales\", values = data[\"Weekly_Sales\"]),\n",
    "                dict(label = \"Holidays\", values = data[\"Holiday_Flag\"]),\n",
    "                dict(label = \"Temperature\", values = data[\"Temperature\"]),\n",
    "                dict(label = \"Fuel price\", values = data[\"Fuel_Price\"]),\n",
    "                dict(label = \"CPI\", values = data[\"CPI\"]),\n",
    "                dict(label = \"Unemployment\", values = data[\"Unemployment\"]),\n",
    "                dict(label = \"Stores\", values = data[\"Store\"])],\n",
    "        marker = dict(color = px.colors.qualitative.Vivid[1], size = 5),\n",
    "        diagonal = dict(visible = False)))\n",
    "\n",
    "# update layout\n",
    "fig2.update_layout(\n",
    "        margin = dict(l = 105, t = 100),\n",
    "        title_text = \"Figure 2. Bivariate analysis\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,   \n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 800)\n",
    "\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3 - exploratory data analysis - plot correlation matrix ### ----\n",
    "\n",
    "# get correlation matrix\n",
    "corr_matrix = data.loc[:,features_num].corr().round(2)\n",
    "\n",
    "# plot correlation matrix\n",
    "fig3 = ff.create_annotated_heatmap(corr_matrix.values,\n",
    "                                  x = corr_matrix.columns.tolist(),\n",
    "                                  y = corr_matrix.index.tolist())\n",
    "\n",
    "# update layout\n",
    "fig3.update_layout(\n",
    "        margin = dict(l = 150, b = 40),\n",
    "        title_text = \"Figure 3. Correlation matrix\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,   \n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig3.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 4 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - preprocessing - clean data ### ----\n",
    "\n",
    "# copy data for safety\n",
    "data1 = data.copy()\n",
    "\n",
    "\n",
    "# part 1 - drop rows where values are missing (for date, weekly sales and holiday flag)\n",
    "\n",
    "# get index of data to drop\n",
    "mask_drop = (data1[\"Date\"].isnull()) | (data1[\"Weekly_Sales\"].isnull()) | (data1[\"Holiday_Flag\"].isnull())\n",
    "index_drop = data1.index[mask_drop]\n",
    "\n",
    "# drop data\n",
    "data1 = data1.drop(index_drop, axis = 0)\n",
    "\n",
    "# print report\n",
    "print(\"Number of rows with missing values that were dropped: {}\"\n",
    "    .format(len(index_drop)))\n",
    "print()\n",
    "\n",
    "# part 2 - create usable features from the Date column and drop date\n",
    "\n",
    "# format date column\n",
    "data1[\"Date\"] = pd.to_datetime(data1[\"Date\"], infer_datetime_format = True)\n",
    "\n",
    "# create features year, month, day, day of week\n",
    "data1[\"year\"] = data1[\"Date\"].dt.year\n",
    "data1[\"month\"] = data1[\"Date\"].dt.month\n",
    "data1[\"day\"] = data1[\"Date\"].dt.day\n",
    "data1[\"day_of_week\"] = data1[\"Date\"].dt.day_of_week\n",
    "data1 = data1.drop([\"Date\"], axis = 1)\n",
    "\n",
    "# display new dataset\n",
    "print(\"Dataset display: \")\n",
    "display(data1.head())\n",
    "print(\"Data shape: {}\".format(data1.shape))\n",
    "print()\n",
    "\n",
    "\n",
    "# part 3 - drop rows containing outliers\n",
    "\n",
    "# set columns to check\n",
    "columns_tocheck = [\"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\"]\n",
    "\n",
    "# initialise variables to store number of rows dropped\n",
    "drop_nb = 0\n",
    "\n",
    "# loop through columns\n",
    "for i in columns_tocheck:\n",
    "\n",
    "    # set bounds to identify outliers\n",
    "    column_mean = data1[i].mean()\n",
    "    column_std = data1[i].std()\n",
    "    lower_bond = column_mean - 3 * column_std\n",
    "    upper_bond = column_mean + 3 * column_std\n",
    "\n",
    "    # get index of rows to drop\n",
    "    mask_drop = (data1[i] < lower_bond) | (data1[i] > upper_bond)\n",
    "    index_drop = data1.index[mask_drop]\n",
    "\n",
    "    # drop rows\n",
    "    data1 = data1.drop(index_drop, axis = 0)\n",
    "\n",
    "    # count rows that were droped\n",
    "    drop_nb += len(index_drop)\n",
    "\n",
    "# print report\n",
    "print(\"Number of rows with outliers that were dropped: {}\".format(drop_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4 - preprocessing - process data for  machine learning ### ----\n",
    "\n",
    "# separate target variable Y from features X\n",
    "X = data1.drop([\"Weekly_Sales\"], axis = 1)\n",
    "Y = data1[\"Weekly_Sales\"]\n",
    "\n",
    "# divide dataset into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# create preprocessor object from pipelines for numeric and categorical features\n",
    "features_num = [\"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\", \"year\", \"month\", \"day\", \"day_of_week\"]\n",
    "features_cat = [\"Store\", \"Holiday_Flag\"]\n",
    "numeric_transformer = Pipeline(steps = [\n",
    "    (\"imputer\", KNNImputer()),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps = [\n",
    "    (\"encoder\", OneHotEncoder(drop = \"first\"))\n",
    "])\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    (\"num\", numeric_transformer, features_num),\n",
    "    (\"cat\", categorical_transformer, features_cat)\n",
    "])\n",
    "\n",
    "# impute missing values and scale numeric features, encode categorical features\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 5 - Baseline model (linear regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5 - baseline model (linear regression) - train model and asess performance ### ----\n",
    "\n",
    "# train model\n",
    "regressor1 = LinearRegression()\n",
    "regressor1.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions on train and test sets\n",
    "Y_train_pred1 = regressor1.predict(X_train)\n",
    "Y_test_pred1 = regressor1.predict(X_test)\n",
    "\n",
    "# perform 5-fold cross-validation to evaluate the generalized R2 score \n",
    "scores1 = cross_val_score(regressor1, X_train, Y_train, cv = 5)\n",
    "print('Cross-validated R2 score: ', scores1.mean())\n",
    "print('Standard deviation: ', scores1.std())\n",
    "print()\n",
    "\n",
    "# assess performance and print report\n",
    "r2_train1 = r2_score(Y_train, Y_train_pred1)\n",
    "r2_test1 = r2_score(Y_test, Y_test_pred1)\n",
    "print(\"R2 score on training set: \", r2_train1)\n",
    "print(\"R2 score on test set: \", r2_test1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 6 - Baseline model feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6 - baseline model feature importance ### ----\n",
    "\n",
    "# get column names from the preprocessor\n",
    "column_names = []\n",
    "for name, pipeline, features_list in preprocessor.transformers_: \n",
    "    if name == 'num': \n",
    "        features = features_list \n",
    "    else: \n",
    "        features = pipeline.named_steps['encoder'].get_feature_names_out() \n",
    "    column_names.extend(features)\n",
    "\n",
    "# store coefficients in a dataframe\n",
    "coefs1 = pd.DataFrame(index = range(0,len(regressor1.coef_)), columns = [\"features\", \"coefficients\"])\n",
    "coefs1[\"features\"] = column_names\n",
    "coefs1[\"coefficients\"] = abs(regressor1.coef_)\n",
    "\n",
    "# get feature importance\n",
    "feature_importance1 = coefs1.sort_values(\"coefficients\", ascending = False).reset_index(drop = True)\n",
    "\n",
    "# plot feature importance\n",
    "fig4 = go.Figure([go.Bar(\n",
    "    x = feature_importance1.loc[:,\"features\"],\n",
    "    y = feature_importance1.loc[:,\"coefficients\"],\n",
    "    marker_color = px.colors.qualitative.Vivid)])\n",
    "\n",
    "# update layout\n",
    "fig4.update_xaxes(tickfont = dict(size = 10), tickangle = 90)\n",
    "fig4.update_yaxes(tickfont = dict(size = 10))\n",
    "fig4.update_layout(\n",
    "        margin = dict(l = 120),\n",
    "        title_text = \"Figure 4. Baseline model feature importance\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        xaxis = dict(title = \"Features\"),\n",
    "        yaxis = dict(title = \"Coefficients\", range = [-50000, 1600000], tickvals = [0, 500000,1000000,1500000]),\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig4.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 7 - Regularized regression model (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7 - regularized regression model (Ridge) ### ----\n",
    "\n",
    "# train model\n",
    "regressor2 = Ridge()\n",
    "regressor2.fit(X_train, Y_train)\n",
    "\n",
    "# make predictions on train and test sets\n",
    "Y_train_pred2 = regressor2.predict(X_train)\n",
    "Y_test_pred2 = regressor2.predict(X_test)\n",
    "\n",
    "# perform 5-fold cross-validation to evaluate the generalized R2 score \n",
    "scores2 = cross_val_score(regressor2, X_train, Y_train, cv = 5)\n",
    "print('Cross-validated R2 score: ', scores2.mean())\n",
    "print('Standard deviation: ', scores2.std())\n",
    "print()\n",
    "\n",
    "# assess performance and print report\n",
    "r2_train2 = r2_score(Y_train, Y_train_pred2)\n",
    "r2_test2 = r2_score(Y_test, Y_test_pred2)\n",
    "print(\"R2 score on training set: \", r2_train2)\n",
    "print(\"R2 score on test set: \", r2_test2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 8 - Regularized model feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8 - regularized model feature importance ### ----\n",
    "\n",
    "# store coefficients in a dataframe\n",
    "coefs2 = pd.DataFrame(index = range(0,len(regressor2.coef_)), columns = [\"features\", \"coefficients\"])\n",
    "coefs2[\"features\"] = column_names\n",
    "coefs2[\"coefficients\"] = abs(regressor2.coef_)\n",
    "\n",
    "# get feature importance\n",
    "feature_importance2 = coefs2.sort_values(\"coefficients\", ascending = False).reset_index(drop = True)\n",
    "\n",
    "# plot feature importance\n",
    "fig5 = go.Figure([go.Bar(\n",
    "    x = feature_importance2.loc[:,\"features\"],\n",
    "    y = feature_importance2.loc[:,\"coefficients\"],\n",
    "    marker_color = px.colors.qualitative.Vivid)])\n",
    "\n",
    "# update layout\n",
    "fig5.update_xaxes(tickfont = dict(size = 10), tickangle = 90)\n",
    "fig5.update_yaxes(tickfont = dict(size = 10))\n",
    "fig5.update_layout(\n",
    "        margin = dict(l = 120),\n",
    "        title_text = \"Figure 5. Regularized model feature importance\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        xaxis = dict(title = \"Features\"),\n",
    "        yaxis = dict(title = \"Coefficients\", range = [-50000, 1600000], tickvals = [0, 500000,1000000,1500000]),\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig5.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 9 - Regularized model fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9 - regularized model fine-tuning ### ----\n",
    "\n",
    "# tune lambda with gridsearch\n",
    "params = {\n",
    "    'alpha': [0, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
    "}\n",
    "gridsearch = GridSearchCV(regressor2, param_grid = params, cv = 5)\n",
    "gridsearch.fit(X_train, Y_train)\n",
    "\n",
    "# print report\n",
    "print(\"Best hyperparameter: \", gridsearch.best_params_)\n",
    "print()\n",
    "\n",
    "# train model\n",
    "regressor3 = Ridge(alpha = gridsearch.best_params_[\"alpha\"])\n",
    "regressor3.fit(X_train, Y_train)\n",
    "\n",
    "# perform 5-fold cross-validation to evaluate the generalized R2 score \n",
    "scores3 = cross_val_score(regressor3, X_train, Y_train, cv = 5)\n",
    "print('Cross-validated R2 score: ', scores3.mean())\n",
    "print('Standard deviation: ', scores3.std())\n",
    "print()\n",
    "\n",
    "# make predictions on train and test sets\n",
    "Y_train_pred3 = regressor3.predict(X_train)\n",
    "Y_test_pred3 = regressor3.predict(X_test)\n",
    "\n",
    "# assess performance and print report\n",
    "r2_train3 = r2_score(Y_train, Y_train_pred3)\n",
    "r2_test3 = r2_score(Y_test, Y_test_pred3)\n",
    "print(\"R2 score on training set: \", r2_train3)\n",
    "print(\"R2 score on test set: \", r2_test3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 10 - Regularized model feature importance after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10 - regularized model feature importance after tuning ### ----\n",
    "\n",
    "# store coefficients in a dataframe\n",
    "coefs3 = pd.DataFrame(index = range(0,len(regressor3.coef_)), columns = [\"features\", \"coefficients\"])\n",
    "coefs3[\"features\"] = column_names\n",
    "coefs3[\"coefficients\"] = abs(regressor3.coef_)\n",
    "\n",
    "# get feature importance\n",
    "feature_importance3 = coefs3.sort_values(\"coefficients\", ascending = False).reset_index(drop = True)\n",
    "\n",
    "# plot feature importance\n",
    "fig6 = go.Figure([go.Bar(\n",
    "    x = feature_importance3.loc[:,\"features\"],\n",
    "    y = feature_importance3.loc[:,\"coefficients\"],\n",
    "    marker_color = px.colors.qualitative.Vivid)])\n",
    "\n",
    "# update layout\n",
    "fig6.update_xaxes(tickfont = dict(size = 10), tickangle = 90)\n",
    "fig6.update_yaxes(tickfont = dict(size = 10))\n",
    "fig6.update_layout(\n",
    "        margin = dict(l = 120),\n",
    "        title_text = \"Figure 6. Regularized model feature importance after tuning\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,\n",
    "        xaxis = dict(title = \"Features\"),\n",
    "        yaxis = dict(title = \"Coefficients\", range = [-50000, 1600000], tickvals = [0, 500000,1000000,1500000]),\n",
    "        showlegend = False,\n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig6.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 11 - Results summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 11 - Results summary ### ----\n",
    "\n",
    "# store results in a dataframe\n",
    "results = pd.DataFrame(index = [\"Linear regression\", \"Ridge lambda=1\", \"Ridge lambda=0.01\"], \n",
    "    columns = [\"Cross-validated R2\", \"R2 standard deviation\", \"R2 train\", \"R2 test\"])\n",
    "results[\"Cross-validated R2\"] = [scores1.mean(), scores2.mean(), scores3.mean()]\n",
    "results[\"R2 standard deviation\"] = [scores1.std(), scores2.std(), scores3.std()]\n",
    "results[\"R2 train\"] = [r2_train1, r2_train2, r2_train3]\n",
    "results[\"R2 test\"] = [r2_test1, r2_test2, r2_test3]\n",
    "\n",
    "# print results summary\n",
    "print(\"Results summary: \")\n",
    "display(results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "### 12 - Store selection and economic indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 12 - store selection and economic indicators ### ----\n",
    "\n",
    "# copy data for safety\n",
    "data2 = data1.copy()\n",
    "\n",
    "# get top 6 stores with the highest coefficients from model3 (coefficients above 1M)\n",
    "stores_desc = [5.0, 3.0, 9.0, 16.0, 7.0, 15.0]\n",
    "\n",
    "# extract data for these stores\n",
    "features_totest = [\"Weekly_Sales\", \"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\"]\n",
    "data2 = data2.loc[data2[\"Store\"].isin(stores_desc),features_totest]\n",
    "\n",
    "# get correlation matrix\n",
    "corr_matrix2 = data2.corr().round(2)\n",
    "\n",
    "# plot correlation matrix\n",
    "fig7 = ff.create_annotated_heatmap(corr_matrix2.values,\n",
    "                                  x = corr_matrix2.columns.tolist(),\n",
    "                                  y = corr_matrix2.index.tolist())\n",
    "\n",
    "# update layout\n",
    "fig7.update_layout(\n",
    "        margin = dict(l = 150, b = 40),\n",
    "        title_text = \"Figure 7. Correlation matrix\",\n",
    "        title_x = 0.5,\n",
    "        title_y = 0.95,\n",
    "        title_font_size = 18,   \n",
    "        plot_bgcolor = \"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor = \"rgb(232,232,232)\",\n",
    "        width = 800,\n",
    "        height = 400)\n",
    "\n",
    "fig7.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6274bd53ad02975ad3a1fc7c53468a83b5e714d6ade680c552f31a0436ec5d39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
